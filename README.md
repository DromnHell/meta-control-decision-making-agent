# meta-control-experiment

## Overview 

This program is the one I developed during my thesis: ["Dromnelle, R. (2021). Architecture cognitive générique pour la coordination de stratégies d'apprentissage en robotique (Doctoral dissertation, Sorbonne université)."](https://www.theses.fr/2021SORUS039) It allows to perform reinforcement-learning tasks in fictitious environments represented by a probabilistic model that describes what happens when the agent acts in the environment.

The main objective of my thesis was to create a meta-control algorithm that allows an agent to coordinate several online behavior strategies. This program allowed me to quickly evaluate several coordination criteria in simulation before performing experiments in the real environment with a real robot. 

## Scripts

* The turtlebotSimulator.py script is the core of the program.

  It takes as input 5 mandatory ordered arguments :
  1.  the id of the experiment that we are going to launch,
  2.  the file that contains the representation of the environment,
  3.  the file that describes the key states of the environment,
  4.  the file that describes the state and action spaces,
  5.  the file that contains the parameters of the agent.
  
  In addition, it can also take 9 optional arguments :
  * the coordination criterion we want to use,
  * the value of the kappa coefficient,
  * the amount of reward expected before the simulation stops,
  * the maximum duration of the simulation,
  * the window size of the filtering,
  * the indication of an upcoming goal change for the agent,
  * the indication of an upcoming environmental change,
  * the record of the data,
  * the record of a compressed version of the data.
  
* The manageEnvironment.py script allows to set up the environment and simulate the actions of the agent on it.
* The utility.py script contains some functions used by other scripts.
* The modelFree.py script allows the agent to use a tabular Q-learning algorithm (model-free behavior) 
to learn to solve the task.
* The modelBased.py script allows the agent to use a tabular Value-Iteration algorithm (model-based behavior) 
to learn to solve the task.
* The DQN.py script allows the agent to use a Deep-Q-Network algorithm (model-free behavior) 
to learn to solve the task.
* The modelBasedReplay.py script allows the agent to use a Prioritized Sweeping algorithm (model-based behavior) 
to learn to solve the task.
* The metaController.py script allows the agent to coordinate the different behavioral strategies implemented in the other scripts.

 ## Other files
 
 * The realisticNavWorld.json file contains a transition model generated by a Turtlebot having explored a navigation arena for 13 hours. This file is like an "image" of the reality used to build the simulated environment.
 * The realisticNavWorld_wall20-21.json file is identical to the realisticNavWorld.json file, with the addition of a wall between the states 20 and 21.
 * The realisticNavWorld_wall6-7.json file is identical to the realisticNavWorld.json file, with the addition of a wall between the states 6 and 7.
 * The realisticNavWorld_afterSwitch.json file is identical to the realisticNavWorld.json file, with a change in the location of the reward, from the state 18 to the state 34.
 * The keyStates.txt file contains the id of the rewarded states and the initial states.
 * The spaces.txt file contains the action space and the states space.
 * The parameters.txt contains the parameters of the agent.
 
## Dependencies

* python 3
* numpy
* tensorflow

## Example of a command to run the program

python turtlebotSimulator.py 0 realisticNavWorld.json keyStates.txt spaces.txt parameters.txt -d 1600


